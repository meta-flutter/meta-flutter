"""
BitBake 'Fetch' implementations

This fetcher is created for gclient.
The main target is flutter-engine, so for other gclient projects, this fetcher might not work.

Copyright (c) 2021-2022 Woven Alpha, Inc
"""

import os
import bb
import datetime
import multiprocessing
import subprocess
import urllib
from   bb.fetch2 import FetchMethod
from   bb.fetch2 import FetchError
from   bb.fetch2 import UnpackError
from   bb.fetch2 import logger
from   bb.fetch2 import runfetchcmd
from   bb.fetch2 import subprocess_setup

class GN(FetchMethod):
    """Class to fetch urls via 'wget'"""
    def supports(self, ud, d):
        """
        Check to see if a given url can be fetched with gn.
        """
        return ud.type in ['gn']

    def recommends_checksum(self, urldata):
        return False

    def supports_checksum(self, urldata):
        return False

    def urldata_init(self, ud, d):
        # syntax: gn://<URL>;name=<NAME>;destdir=<D>;proto=<PROTO>
        name = ud.parm.get("name", "src")
        # URI "name=" is special, can't have path slashes, otherwise we
        # risk parse errors with things like
        # SRC_URI[src/flutter.sha256sum].
        # So always prepend the checkout path with "src/" when not using
        # the default.
        if not name.startswith("src"):
            name = os.path.join("src/", name)

        ud.destdir = "" if "destdir" not in ud.parm else ud.parm["destdir"]
        proto = "https" if "proto" not in ud.parm else ud.parm["proto"]

        ud.basename = "*"

        custom_vars = d.getVar("GN_CUSTOM_VARS")
        sync_opt = d.getVar("EXTRA_GN_SYNC")
        if d.getVar("BB_GIT_SHALLOW") == "1":
            sync_opt += " --no-history --shallow"

        depot_tools_path = d.getVar("DEPOT_TOOLS")
        python2_path = d.getVar("PYTHON2_PATH")

        uri = ud.url.split(";")[0].replace("gn://", "%s://" % (proto))
        gclient_config = '''solutions = [
    {
        "managed": False,
        "name": "%s",
        "url": "%s",
        "custom_vars": %s
    }
]''' % (name, uri, custom_vars)

        srcrev = d.getVar("SRCREV")
        dl_dir = d.getVar("DL_DIR")
        gndir = os.path.join(dl_dir, "gn")
        ud.syncdir = uri.replace(":", "").replace("/", "_")
        ud.clonedir = os.path.join(gndir, ud.syncdir)
        ud.localfile = ud.clonedir
        mirrortarball = ud.syncdir + "-" + srcrev + ".tar.bz2"
        ud.fullmirror = os.path.join(d.getVar("DL_DIR"), mirrortarball)
        ud.mirrortarballs = [mirrortarball]

        # It would probably be wise to run gclient.py sync with --nohooks.
        # This would help with keeping things more reproducible.
        # The downside is we'd have to run the hooks later to generate
        # metadata.
        ud.basecmd = "export PATH=\"%s:%s:${PATH}\"; export DEPOT_TOOLS_UPDATE=0; export GCLIENT_PY3=0; \
            export CURL_CA_BUNDLE=%s; \
            gclient.py config --spec '%s' && \
            gclient.py sync %s --revision %s %s -v && " % (
                depot_tools_path, os.path.join(depot_tools_path, python2_path),
                d.getVar('CURL_CA_BUNDLE'),
                gclient_config,
                sync_opt, srcrev, d.getVar('PARALLEL_MAKE'))

        # It is possible to make the resulting clone and mirror tar file
        # reproducible.  The problem: the flutter build depends on the .git
        # directories being present, as it does a git rev-parse.  Since we
        # can't get rid of the changing .git dirs, then we have to live
        # with non-reproducibility.
        # We do make some adjustments to remove or adjust timestamps in the
        # hopes that it may help sstate cache when building.  Can't hurt.
        # If one wishes to be more agressive, they could remove
        # .cipd/tagcache (non-sorted binary cache file), as well as the
        # generated PNG files in
        # ./src/flutter/testing/scenario_app/ios/Scenarios/ScenariosUITests.

        # The .packages files have timestamps in generated comments.  We
        # remove them.
        ud.basecmd += "find . -type f -name .packages | \
            xargs sed -i 's/^# Generated by pub on .*//' && "

        # package_config.json files also have timestamps stored in the
        # "generated" key, e.g.:
        # "generated": "2023-03-06T21:39:21.029153Z",
        # We use SOURCE_DATE_EPOCH to re-generate them.
        source_date_epoch = d.getVar("SOURCE_DATE_EPOCH").strip()
        source_date = datetime.datetime.fromtimestamp(
            int(source_date_epoch))
        source_date_str = source_date.isoformat() + "Z"
        ud.basecmd += "find . -name package_config.json | \
            xargs sed -i 's/^  \"generated\": \".*Z\",/  \"generated\": \"%s\",'/" % (
            source_date_str)

        bb_number_threads = d.getVar("BB_NUMBER_THREADS", multiprocessing.cpu_count()).strip()

        # Tar reproducibility note:
        # If the flutter build stops using git rev-parse, then we could use
        # "--exclude-vcs" for tar reproducibility.  Also, pbzip2 does not
        # yield reproducible archives.  Maybe someday it will.

        # Tar reproducibility command modeled off of:
        # https://reproducible-builds.org/docs/archives/ - see "Full
        # example" section.
        ud.packcmd = "tar -I \"pbzip2 -p%s\" \
            --sort=name --mtime='@%s' \
            --owner=0 --group=0 --numeric-owner \
            --pax-option=exthdr.name=%%d/PaxHeaders/%%f,delete=atime,delete=ctime \
            -cf %s ./" % ( bb_number_threads, source_date_epoch,
                ud.fullmirror)

        ud.write_tarballs = d.getVar("BB_GENERATE_MIRROR_TARBALLS")

    def _rungnclient(self, ud, d, quiet):
        bb.utils.mkdirhier(ud.clonedir)
        os.chdir(ud.clonedir)

        logger.debug(2, "Fetching %s using command '%s'" % (ud.url, ud.basecmd))
        bb.fetch2.check_network_access(d, ud.basecmd, ud.url)
        runfetchcmd(ud.basecmd, d, quiet, workdir=None)

    def localpath(self, ud, d):
        return ud.clonedir

    def try_premirror(self, ud, d):
        # If we don't do this, updating an existing checkout with only premirrors
        # is not possible
        if bb.utils.to_boolean(d.getVar("BB_FETCH_PREMIRRORONLY")):
            return True
        if os.path.exists(ud.localfile):
            return False
        return True

    def download(self, ud, d):
        """Fetch urls"""
        # If tar.bz2 exists, skip
        if os.path.exists(ud.fullmirror) and os.access(ud.fullmirror, os.R_OK):
            # This is really subtle.  When in a premirror situation,
            # localpath needs to change to the tar file in order for the
            # fetcher to know that it already downloaded the file.
            ud.localpath = ud.fullmirror
            return True

        uri = ud.url.split(";")[0]

        self._rungnclient(ud, d, False)

        # Sanity check since wget can pretend it succeed when it didn't
        # Also, this used to happen if sourceforge sent us to the mirror page
        if not os.path.exists(ud.clonedir):
            raise FetchError("The fetch command returned success for url %s but %s doesn't exist?!" % (uri, ud.clonedir), uri)

        return True

    def build_mirror_data(self, ud, d):
        # Generate a mirror tarball if needed
        if ud.write_tarballs == "1" and not os.path.exists(ud.fullmirror):
            # it's possible that this symlink points to read-only filesystem with PREMIRROR
            if os.path.islink(ud.fullmirror):
                os.unlink(ud.fullmirror)

            logger.info("Creating tarball of gn fetch dir")
            logger.debug(2, "Packing %s using command '%s'" % (ud.url, ud.packcmd))
            runfetchcmd(ud.packcmd, d, workdir=None)
            runfetchcmd("touch %s.done" % (ud.fullmirror), d)

    def unpack(self, ud, workdir, d):
        unpackdir = os.path.join(workdir, ud.destdir)

        # untar the archive to the right spot.
        if os.path.exists(ud.fullmirror):
            bb_number_threads = d.getVar("BB_NUMBER_THREADS", multiprocessing.cpu_count()).strip()
            cmd = 'pbzip2 -dc -p%s %s | tar x --no-same-owner -f -' % (
                bb_number_threads, ud.fullmirror)
            path = d.getVar('PATH')
            if path:
                cmd = "PATH=\"%s\" %s" % (path, cmd)
            bb.note("Unpacking %s to %s" % (ud.fullmirror, unpackdir))

        # Check if the "raw" clone dir exists, if so, copy wholesale.
        elif os.path.exists(ud.clonedir):
            bb.note("Unpacking by copying directory content from %s to %s." %
                (ud.clonedir, unpackdir))
            cmd = "tar -C %s -cf - . | tar --no-same-owner -xf -" % ud.clonedir

        else:
            bb.error("Neither clonedir %s or fullmirror %s exist." % (ud.clonedir, ud.fullmirror))
            ret = -1
            cmd = "null"

        bb.utils.mkdirhier(unpackdir)
        ret = subprocess.call(cmd, preexec_fn=subprocess_setup, shell=True, cwd=unpackdir)

        if ret != 0:
            raise UnpackError("Unpack command %s failed with return value %s" % (cmd, ret), ud.url)

        return


    def clean(self, ud, d):
        bb.utils.remove(ud.clonedir, recurse=True)
        bb.utils.remove(ud.fullmirror)
        bb.utils.remove(ud.fullmirror + ".done")

